{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "aed1f1e4-8d87-47e0-8b29-a35abb25ffec",
   "metadata": {},
   "outputs": [],
   "source": [
    "from astropy.coordinates import SkyCoord\n",
    "from astropy.coordinates.name_resolve import NameResolveError\n",
    "from astropy.table import Table, Row, vstack, unique, hstack\n",
    "from astropy.time import Time\n",
    "from astropy import units as u\n",
    "\n",
    "from astroquery.vizier import Vizier\n",
    "\n",
    "import numpy\n",
    "\n",
    "from scipy.spatial.distance import cdist\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "import itertools\n",
    "import pickle\n",
    "\n",
    "from targets import ESCAPED_TARGET_NAMES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daf55d63-c5ba-4711-b16b-d675621dccbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"//stem-linux-homes/OSL-Telescope/data/users/Pipeline/\")\n",
    "NUM_CALIBRATION_ROWS = 10\n",
    "CALIBRATION_CATALOGUES = {\n",
    "    'I/284/out': {\n",
    "        'B': 'B1mag',\n",
    "        'R': 'R1mag',\n",
    "        'I': 'Imag',\n",
    "    },\n",
    "    'I/297/out': {\n",
    "        'B': 'Bmag',\n",
    "        'V': 'Vmag',\n",
    "        'R': 'Rmag',\n",
    "    },\n",
    "    'VI/135/table15': {\n",
    "        'B': 'BTmag',\n",
    "        'V': 'VTmag',\n",
    "    },\n",
    "    'II/339/uvotssc1': {\n",
    "        'U': 'Umag',\n",
    "        'B': 'Bmag',\n",
    "        'V': 'Vmag',\n",
    "    }\n",
    "}\n",
    "REPROCESS = False # Set to True to reprocess previous data rather than finding new data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8241d12c-3c83-433d-b26c-4a04e885eac0",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    with open('data/processed_dates.pickle', 'rb') as processed_dates_file:\n",
    "        processed_dates = pickle.load(processed_dates_file)\n",
    "except FileNotFoundError:\n",
    "    processed_dates = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7680671b-28fa-4910-b471-a5f4f85bfbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "try: \n",
    "    with open('data/coords.pickle', 'rb') as coords_file:\n",
    "        coords = pickle.load(coords_file)\n",
    "except FileNotFoundError:\n",
    "    coords = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "179a6a9d-13e7-4add-aea8-c9d21e59df21",
   "metadata": {},
   "outputs": [],
   "source": [
    "if REPROCESS:\n",
    "    processed_dates = [ d for d, has_data in processed_dates.items() if not has_data ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "28ababc2-2d96-4505-99ef-fbbcaad05afe",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1e12e6571584cf3a160b6de135643f4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "New dates:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "62e1627785334f2f837fc68fe3f1e386",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "\\\\stem-linux-homes\\OSL-Telescope\\data\\users\\Pipeline\\COAST\\2021_11_04: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "56b296c532994210adc126e798ec7efc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "\\\\stem-linux-homes\\OSL-Telescope\\data\\users\\Pipeline\\PIRATE\\2021_11_04: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "new_dates = [p for p in DATA_PATH.glob('*/202?_??_??') if p not in processed_dates]\n",
    "total_dates = len(new_dates)\n",
    "obs_tables = {}\n",
    "\n",
    "SPLIT_SIZE = 10\n",
    "\n",
    "for date in tqdm(new_dates, desc='New dates'):\n",
    "    date_has_data = False\n",
    "    for catalogue in tqdm(list(date.glob('Catalogues/*_anm83_*.cat')), desc=str(date)):\n",
    "        date_has_data = True\n",
    "        name = None\n",
    "        for escaped_target_name, target_name in ESCAPED_TARGET_NAMES.items():\n",
    "            if escaped_target_name in catalogue.stem:\n",
    "                name = target_name\n",
    "                obs_meta = list(itertools.chain.from_iterable(l.split('_') for l in catalogue.stem.split(f'_{escaped_target_name}_')))\n",
    "                break\n",
    "        if name is None:\n",
    "            continue\n",
    "        obs_meta = {\n",
    "            'telescope': obs_meta[0],\n",
    "            'name': name,\n",
    "            'band': obs_meta[6][0],\n",
    "            'exposure': float(obs_meta[6][1:]),\n",
    "            'timestamp': Time(\n",
    "                dict(zip(\n",
    "                    ['year', 'month', 'day', 'hour', 'minute', 'second'],\n",
    "                    map(int, obs_meta[8:14])\n",
    "                )),\n",
    "                format='ymdhms',\n",
    "            ).jd,\n",
    "        }\n",
    "        if obs_meta['name'] not in coords:\n",
    "            try:\n",
    "                coords[obs_meta['name']] = SkyCoord.from_name(obs_meta['name'], parse=True)\n",
    "            except NameResolveError:\n",
    "                continue\n",
    "        \n",
    "        table = Table.read(catalogue, format='ascii.sextractor')\n",
    "        table.rename_column('ALPHA_J2000', 'RA')\n",
    "        table.rename_column('DELTA_J2000', 'Dec')\n",
    "        \n",
    "        table['separation'] = SkyCoord.guess_from_table(table).separation(coords[obs_meta['name']])\n",
    "        table.sort('separation') #To do: use idxmin here instead of sorting\n",
    "        target_row = table[0]\n",
    "        out_table = Table(target_row)\n",
    "        \n",
    "        table.rename_column('RA', '_RAJ2000')\n",
    "        table.rename_column('Dec', '_DEJ2000')\n",
    "        \n",
    "        calibration_succeeded = False\n",
    "        \n",
    "        Path(f\"data/calibration_tables/{obs_meta['name']}\").mkdir(parents=True, exist_ok=True)\n",
    "        for calibration_catalogue, calibration_catalogue_fields in CALIBRATION_CATALOGUES.items():\n",
    "            if obs_meta['band'] in calibration_catalogue_fields:\n",
    "                calibration_catalogue_path = f\"data/calibration_tables/{obs_meta['name']}/{calibration_catalogue.replace('/', '_')}_{catalogue.stem}.ecsv\"\n",
    "                try:\n",
    "                    calibration_table = Table.read(calibration_catalogue_path)\n",
    "                except FileNotFoundError:\n",
    "                    table = table[table['FLAGS'] == 0]\n",
    "                    table['flux_diff'] = numpy.abs(table['FLUX_AUTO'] - target_row['FLUX_AUTO'])\n",
    "                    table.sort('flux_diff')\n",
    "                    table = table[:50]\n",
    "                    table.sort('separation')\n",
    "\n",
    "                    calibration_rows = []\n",
    "\n",
    "                    total_iterations = int(len(table) / SPLIT_SIZE) + 1\n",
    "                    for i in range(total_iterations):\n",
    "                        sources = table[i * SPLIT_SIZE : (i+1) * SPLIT_SIZE]\n",
    "                        if len(sources) == 0:\n",
    "                            continue\n",
    "                        sources = vstack([ row for row in sources if row['NUMBER'] != target_row['NUMBER'] ])\n",
    "                        catalogue_results = Vizier.query_region(sources, radius=1e-4*u.deg, catalog=calibration_catalogue)\n",
    "                        if len(catalogue_results) == 0:\n",
    "                            continue\n",
    "\n",
    "                        # Reject any sources with multiple matches\n",
    "                        catalogue_matches = unique(catalogue_results[0], '_q', keep='none')\n",
    "\n",
    "                        # To do: Reject any known variables\n",
    "\n",
    "                        for catalogue_row in catalogue_matches[~numpy.isnan(catalogue_matches[calibration_catalogue_fields[obs_meta['band']]])]:\n",
    "                            calibration_rows.append(hstack([\n",
    "                                sources[int(catalogue_row['_q']) - 1],\n",
    "                                catalogue_row[[calibration_catalogue_fields[obs_meta['band']]]],\n",
    "                            ]))\n",
    "\n",
    "                        if len(calibration_rows) >= NUM_CALIBRATION_ROWS:\n",
    "                            break\n",
    "                    if len(calibration_rows) < NUM_CALIBRATION_ROWS:\n",
    "                        continue\n",
    "                    calibration_table = vstack(calibration_rows[:NUM_CALIBRATION_ROWS], metadata_conflicts='silent')\n",
    "                    calibration_table.write(\n",
    "                        calibration_catalogue_path,\n",
    "                        overwrite=True,\n",
    "                    )\n",
    "\n",
    "                calibrated_mags = (\n",
    "                    calibration_table[calibration_catalogue_fields[obs_meta['band']]] \n",
    "                    - 2.5 * numpy.log10(target_row['FLUX_AUTO'] / calibration_table['FLUX_AUTO'])\n",
    "                )\n",
    "\n",
    "                # To do: Maybe this should be a weighted average using the flux error?\n",
    "                out_table.add_columns(\n",
    "                    [\n",
    "                        numpy.mean(calibrated_mags),\n",
    "                        # Standard error as per https://www.statology.org/standard-error-of-mean-python/\n",
    "                        numpy.std(calibrated_mags, ddof=1) / numpy.sqrt(numpy.size(calibrated_mags)),\n",
    "                        calibration_catalogue,\n",
    "                    ],\n",
    "                    names=['calibrated_mag', 'calibrated_mag_err', 'calibration_catalogue'],\n",
    "                )\n",
    "                calibration_succeeded = True\n",
    "                break\n",
    "\n",
    "        if not calibration_succeeded:\n",
    "            out_table.add_columns(\n",
    "                [\n",
    "                    numpy.nan,\n",
    "                    numpy.nan,\n",
    "                    '',\n",
    "                ],\n",
    "                names=['calibrated_mag', 'calibrated_mag_err', 'calibration_catalogue'],\n",
    "            )\n",
    "        \n",
    "        for key, val in obs_meta.items():\n",
    "            out_table[key] = val\n",
    "        \n",
    "        if obs_meta['name'] not in obs_tables:\n",
    "            try:\n",
    "                if REPROCESS:\n",
    "                    obs_tables[obs_meta['name']] = out_table\n",
    "                    continue\n",
    "                else:\n",
    "                    obs_tables[obs_meta['name']] = Table.read(f\"data/{obs_meta['name']}.ecsv\")\n",
    "            except FileNotFoundError:\n",
    "                obs_tables[obs_meta['name']] = out_table\n",
    "                continue\n",
    "        obs_tables[obs_meta['name']] = vstack([obs_tables[obs_meta['name']], out_table])\n",
    "        \n",
    "    if not REPROCESS:\n",
    "        processed_dates[date] = date_has_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03cc5012-89c8-4a60-99cd-38db9312a047",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, table in obs_tables.items():\n",
    "    table.write(f\"data/{name}.ecsv\", overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "27e2fe8f-c6c6-454d-ac97-460fc5c9fdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not REPROCESS:\n",
    "    with open('data/processed_dates.pickle', 'wb') as processed_dates_file:\n",
    "        pickle.dump(processed_dates, processed_dates_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "704b847a-dbc3-4fc2-8ce4-ecf1fe1c6dab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data/coords.pickle', 'wb') as coords_file:\n",
    "    pickle.dump(coords, coords_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfa27474-870e-463a-a140-7c9d777bb090",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
